# LORA技术笔记

##  0 x 00 Intro

LoRA (Low-Rank Adaptation) 指的是冻结预训练模型权重，并在每个Transformer块中注入可训练层（秩分解矩阵），通过在模型的Linear Layer旁边加两个模块，A和B。A将数据从d维降到r维，r是LoRA的秩。B将数据从r维升到d维，B部分的参数初始为0。

![img](https://img-blog.csdnimg.cn/direct/7badde72bcf84a619ecb279249253e12.png)

## 0 x 01 Pourquoi avons - nous besoin de Lora?

大模型的微调成本和部署成本极高，如果根据不同下游任务微调多个模型就会需要针对不同的用户微调不同的模型，成本高。

## 0 x 02 À propos de la résolution de Lora

- 重参数
  - 也就是结构重参数化，先构造原始网络结构，在将其权重参数等价转换为另一组参数。
- 本征维度
  - 指的是一个数据集的有效维度或者说高效率维度的数量，即可以用最少的维度来表达数据集最多的信息。确定一个数据集的本征维度一般使用主成成分分析，独立成分分析，多维缩放。
  - 预训练的效果越好，模型表征能力越强，泛化性越强，本征维度越小
- LoRA原理
  - 在尽量不改变推理速度前提下，使用少量数据让大模型达到原始推理精度90%以上，从而来实现各种下游任务。也就是说，网络结构不可变，即**重参数**不可变。神经网络由一个个的层组成，每一层都是执行矩阵乘法的单元，这些层的权重矩阵通常具有全秩（全秩 : 每一个行向量/列向量线性无关）。
  - LoRA就是冻结预训练模型权重，将可训练的秩分解矩阵注入到Transformer层的权重中，减少下游任务训练所需要的参数量，就是在旁支增加Linear_A将d降到r，再用Linear_B将r升到d。
    - d ： 预训练模型输入输出维度。
    - r ： 秩
    - A ：随机的高斯初始化
    - B ： 零矩阵